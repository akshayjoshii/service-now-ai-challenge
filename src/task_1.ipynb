{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Content Summarization\n",
    "\n",
    "Build a model to classify if two sentences are paraphrases of each other. “1” = yes, “0” = no. You are expected to establish an end-to-end process, including pre-processing, modeling, validation, etc.\n",
    "\n",
    "\n",
    "**IMPORTANT:** Please make sure you are in the '/project_root/src/' directory when running this notebook. Further, install the required packages by running the following command in the terminal: `pip install -r requirements.txt`\n",
    "\n",
    "#### Quroa Question Pairs Dataset Stats:\n",
    "\n",
    "1. Available Columns: id, qid1, qid2, question1, question2, is_duplicate\n",
    "\n",
    "2. Class labels: 0 (not paraphrases), 1 (paraphrases/duplicates)\n",
    "\n",
    "3. Total training data / No. of rows: 404290\n",
    "\n",
    "4. No. of columns: 6\n",
    "\n",
    "5. No. of non-duplicate data points is 255027\n",
    "\n",
    "6. No. of duplicate data points is 149263\n",
    "\n",
    "7. We have 404290 training data points. And only 36.92% are positive. That means it is an **imbalanced** dataset.\n",
    "\n",
    "\n",
    "#### Losses Available to build such a model:\n",
    "\n",
    "1. **Cosine Embedding Loss:** Similar pairs with label 1 are pulled together, so that they are close in vector space. Dissimilar pairs, that are closer than a defined margin, are pushed away in vector space. Metric used is cosine distance.\n",
    "\n",
    "2. **Online Contrastive Loss:** improved version of cosine emb. loss. Looks which negative pairs have a lower distance that the largest positive pair and which positive pairs have a higher distance than the lowest distance of negative pairs. I.e., this loss automatically detects the hard cases in a batch and computes the loss only for these cases.\n",
    "\n",
    "3. **Multiple Negatives Ranking Loss:** reduces the distance between positive pairs out of large set of possible candidates. However, the distance between non-duplicate questions is not so large, so that this loss does not work that weill for pair classification.\n",
    "\n",
    "4. **MSE:** problematic as the loss does not\n",
    "take the relative order into account. For instance, for two pairs with correct target scores (0.4, 0.5), the loss function would equally penalize answers like (0.3, 0.6) and (0.5, 0.4). However, the first pair is better, as it keeps the correct ranking, while the second one does not.\n",
    "\n",
    "5. **Triplet Loss & InfoNCE:** there is a problem\n",
    "for constructing pairs or triplets in the training set, as it is hard to find non-trivial negatives examples.\n",
    "\n",
    "6. **Batch Softmax Contrastive Loss:** Best of all & latest, but too complex to implement & no open source code available.\n",
    "\n",
    "7. **Cross Entropy Loss (Easiest of the above):** this is the most simplest & effective loss functions among all the above. It is also the most widely used loss function for classification tasks. But, if we use this, then only the final linear layer will learn the mapping to output distribution really well & the rest of the layers in the model might not learn good representations of the sentences. Further, zero-shot formulation during inference is not possible with this loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VERY IMPORTANT:** It is advised to rather use the **task_1_run_me.py** script to seriously train & evaluate the model on multiple GPU's. Jupyter Notebooks are slow & only useful for demonstration purposes. Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/joshi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging training progress using wandb...Please make sure to login to wandb using your API key.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makshayjoshi\u001b[0m (\u001b[33makshay-joshi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/joshi/.netrc\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to import all the required custom functions and classes\n",
    "from utils.dataset_utils import (\n",
    "    get_quora_dataset, \n",
    "    create_k_fold_datasets,\n",
    "    load_df_from_pickle\n",
    ")\n",
    "\n",
    "from utils.preprocess_utils import normalize_text\n",
    "from utils.plot import plot_zipf_distribution\n",
    "\n",
    "from utils.dataloader import get_dataset_generators\n",
    "from utils.model import AkshayFormer\n",
    "from utils.trainer import AdapterTransfomerTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "quora = get_quora_dataset(\n",
    "            num_samples=50000,\n",
    "            balanced=True,\n",
    "            rand_seed=7\n",
    "        )\n",
    "\n",
    "# Split into 3 folds for fair evaluation\n",
    "create_k_fold_datasets(\n",
    "            dataset=quora, \n",
    "            num_folds=5,\n",
    "            rand_seed=7\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load df from pickle file\n",
    "df1 = load_df_from_pickle(\"data/cross_folds/train_1_folds.pkl\")\n",
    "df2 = load_df_from_pickle(\"data/cross_folds/test_1_folds.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing operations on the text & normalize the text\n",
    "df1_norm = normalize_text(df1)\n",
    "df2_norm = normalize_text(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Zipfs curve for non-normalized text\n",
    "plot_zipf_distribution(\n",
    "    dataframes=(df1, df2),\n",
    "    title=\"Zipf's Curve for Custom Quora Dataset: Before Normalization\", \n",
    "    save_path=\"plots/zipfs_curve_unnormalized.png\"\n",
    ")\n",
    "\n",
    "# Plot Zipfs curve for normalized text\n",
    "plot_zipf_distribution(\n",
    "    dataframes=(df1_norm, df2_norm),\n",
    "    title=\"Zipf's Curve for Custom Quora Dataset: After Normalization\", \n",
    "    save_path=\"plots/zipfs_curve_normalized.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the AkshayFormer model & corresponding trainer to train in a self-supervised way\n",
    "# using Pairwise Cosine Embedding Contrastive Loss\n",
    "model = AkshayFormer(model_name_or_path=\"thenlper/gte-base\")\n",
    "trainer = AdapterTransfomerTrainer(\n",
    "                model=model,\n",
    "                epochs=50,\n",
    "                learning_rate=0.05,\n",
    "                train_full_model=True,\n",
    "                model_save_name='AkshayFormer_AO_CV1',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset generators to sample tuples of anchor, positive/negative, label\n",
    "train_dset, val_dset, test_dset = get_dataset_generators(\n",
    "                                    train_df=df1,\n",
    "                                    test_df=df2,\n",
    "                                    model_name_or_path=\"thenlper/gte-base\",\n",
    "                                    max_seq_length=64,\n",
    "                                    seed=55\n",
    "                                )\n",
    "\n",
    "# Get the dataloaders\n",
    "train_loader, test_loader, val_loader = trainer.get_data_loaders(\n",
    "                                            train_dset,\n",
    "                                            test_dset,\n",
    "                                            val_dset,\n",
    "                                            batch_size=256\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model. This single method will also evaluate the model on the validation \n",
    "# set after each epoch and later test on test set after the training is complete.\n",
    "trainer.train(\n",
    "        train_loader, \n",
    "        val_loader,\n",
    "        test_loader\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "servicenow_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
