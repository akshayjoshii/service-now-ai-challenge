{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quroa Question Pairs Dataset Stats:**\n",
    "\n",
    "Available Columns: id, qid1, qid2, question1, question2, is_duplicate\n",
    "\n",
    "Class labels: 0 (not paraphrases), 1 (paraphrases/duplicates)\n",
    "\n",
    "Total training data / No. of rows: 404290\n",
    "\n",
    "No. of columns: 6\n",
    "\n",
    "No. of non-duplicate data points is 255027\n",
    "\n",
    "No. of duplicate data points is 149263\n",
    "\n",
    "We have 404290 training data points. And only 36.92% are positive. That means it is an **imbalanced** dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses\n",
    "1. **Cosine Embedding Loss:** Similar pairs with label 1 are pulled together, so that they are close in vector space. Dissimilar pairs, that are closer than a defined margin, are pushed away in vector space. Metric used is cosine distance.\n",
    "2. **Online Contrastive Loss:** improved version of cosine emb. loss. Looks which negative pairs have a lower distance that the largest positive pair and which positive pairs have a higher distance than the lowest distance of negative pairs. I.e., this loss automatically detects the hard cases in a batch and computes the loss only for these cases.\n",
    "3. **Multiple Negatives Ranking Loss:** reduces the distance between positive pairs out of large set of possible candidates. However, the distance between non-duplicate questions is not so large, so that this loss does not work that weill for pair classification.\n",
    "4. **MSE:** problematic as the loss does not\n",
    "take the relative order into account. For instance, for two pairs with correct target scores (0.4, 0.5), the loss function would equally penalize answers like (0.3, 0.6) and (0.5, 0.4). However, the first pair is better, as it keeps the correct ranking, while the second one does not.\n",
    "5. **Triplet Loss & InfoNCE:** there is a problem\n",
    "for constructing pairs or triplets in the training set, as it is hard to find non-trivial negatives examples.\n",
    "6. **Batch Softmax Contrastive Loss:** Best of all & latest, but too complex to implement & no open source code available."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
